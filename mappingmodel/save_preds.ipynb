{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Once we have trained a model, it's important to visualize its predictions. We're going to use R to make our visualizations, but our model has been saved as a python object. This script helps with the transition betwen languages, saving all the model's predictions as numpy arrays, which can be read in R using the `reticulate` package. We'll make predictions on both training and test data, to gauge the degree of over / underfitting.\n",
    "\n",
    "2. The block below defines some high-level parameters for this script. If you are running this on your own machine, you should change the `data_dir` parameter to whereever you have been storing the raw and processed data. Also, if you have access to a GPU, you should change the `device` parameter, since it would help us get the predictions more quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"/home/jovyan/\")\n",
    "process_dir = data_dir / \"processed\"\n",
    "args = {\n",
    "    \"device\": \"cpu\", # set to \"cuda\" if gpu is available\n",
    "    \"out_dir\": data_dir / \"predictions\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We left the last notebook without fully training the model. We also never generated the test data that would have been made by the `2-preprocessing.Rmd` script before. Instead, in this block, we will download a test data set and trained model, currently stored in a UW Madison box folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import download_data\n",
    "\n",
    "links = {\n",
    "    \"test_data\": \"https://uwmadison.box.com/shared/static/zs8vtmwbl92j5oq6ekzcfod11ym1w599.gz\",\n",
    "    \"model\": \"https://uwmadison.box.com/shared/static/byb5lpny6rjr15zbx28o8liku8g6nga6.pt\"\n",
    "}\n",
    "\n",
    "download_data(links[\"test_data\"], process_dir / \"test.tar.gz\")\n",
    "download_data(links[\"model\"], data_dir / \"model.pt\", unzip = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. This block sets up the model that we just downloaded. The `.eval()` step specifies that we are no longer using the model for training. We don't need to keep track of model gradients anymore, since all we care about are predictions made with the existing weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from unet import Unet\n",
    "\n",
    "state = torch.load(data_dir / \"model.pt\", map_location=args[\"device\"])\n",
    "model = Unet(13, 3, 5).to(args[\"device\"])\n",
    "model.load_state_dict(state)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. The block below creates `Dataset` objects from which we can load the preprocessed training and test samples. We rely on the fact that our directory structure completely species the train / test split. We will iterate over these images one by one, saving a prediction for each. In principle, it's possible to save predictions over batches of images by first defining a data loader. This would be a bit more complex to implement, though, and we're aiming for simplicity here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import GlacierDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "paths = {}\n",
    "for split in [\"train\", \"test\"]:\n",
    "    paths[split] = {}\n",
    "    for v in [\"x\", \"y\"]:\n",
    "        paths[split][v] = list((process_dir / split).glob(v + \"*\"))\n",
    "        paths[split][v].sort()\n",
    "\n",
    "ds = {\n",
    "    \"train\": GlacierDataset(paths[\"train\"][\"x\"], paths[\"train\"][\"y\"]),\n",
    "    \"test\": GlacierDataset(paths[\"test\"][\"x\"], paths[\"test\"][\"y\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Finally, we save predictions to the `args[\"out_dir\"]` folder. The code for the `predictions` function is given in the `train.py` script. It iterates over the loader and saves a numpy array with predictions for each sample. Somewhat counterintuively, we also save the `x` and `y`'s associated with each prediction. The reason is that the output from the `Dataset` object is not deterministic -- we may return a random rotation or flip of the original image. This was done to encourage invariance to these transformations in our model, but makes it hard to compare the predictions directly with the objects in the `processed` directory. By writing all the matched input, label, and prediction data again at this point, we make it easier to study the specific version of the inputs that are related to good and bad model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import predictions\n",
    "\n",
    "predictions(model, ds[\"train\"], args[\"out_dir\"] / \"train\", args[\"device\"])\n",
    "predictions(model, ds[\"test\"], args[\"out_dir\"] / \"test\", args[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mappingvis] *",
   "language": "python",
   "name": "conda-env-mappingvis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
